{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
    "from sklearn.decomposition import MiniBatchDictionaryLearning\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check if TensorFlow can access the GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"GPU is NOT available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 64)\n"
     ]
    }
   ],
   "source": [
    "def learn_dictionary_from_digits():\n",
    "    \"\"\"\n",
    "    Load the digits dataset and learn an over-determined dictionary from it using MiniBatchDictionaryLearning.\n",
    "    \n",
    "    Returns:\n",
    "        dictionary: numpy array\n",
    "            The learned dictionary.\n",
    "        dict_learner: MiniBatchDictionaryLearning object\n",
    "            The trained dictionary learner.\n",
    "    \"\"\"\n",
    "    # Load the digits dataset\n",
    "    digits = load_digits()\n",
    "    images = digits.data\n",
    "    \n",
    "    # Set the number of components (atoms) for the dictionary to ensure it's over-determined\n",
    "    n_components = 100  # Greater than 64 (the dimensionality of the input data)\n",
    "    \n",
    "    # Use MiniBatchDictionaryLearning to learn a dictionary\n",
    "    dict_learner = MiniBatchDictionaryLearning(n_components=n_components, alpha=1, random_state=0, n_jobs=-1, transform_algorithm='omp',)\n",
    "    dictionary = dict_learner.fit(images).components_\n",
    "    \n",
    "    return dictionary, dict_learner\n",
    "\n",
    "# Execute the function to get the learned dictionary and the dictionary learner object\n",
    "dictionary, dict_learner = learn_dictionary_from_digits()\n",
    "print(dictionary.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_with_OMP(dictionary, images, dict_learner):\n",
    "    \"\"\"\n",
    "    Reconstruct the dataset using MiniBatchDictionaryLearning's transform method and a given dictionary.\n",
    "    \n",
    "    Args:\n",
    "        dictionary: numpy array\n",
    "            The learned dictionary.\n",
    "        images: numpy array\n",
    "            The dataset to be reconstructed.\n",
    "        dict_learner: MiniBatchDictionaryLearning object\n",
    "            The trained dictionary learner.\n",
    "    \n",
    "    Returns:\n",
    "        reconstructed: numpy array\n",
    "            The reconstructed dataset.\n",
    "    \"\"\"\n",
    "    # Get the sparse representation of the images using the dictionary learner\n",
    "    coefficients = dict_learner.transform(images)\n",
    "\n",
    "    # print(coefficients.shape)\n",
    "    dictionary = dictionary\n",
    "    \n",
    "    # Reconstruct the images from the coefficients and the dictionary\n",
    "    reconstructed = np.dot(coefficients, dictionary)\n",
    "    return reconstructed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def average_ssim(original, reconstructed):\n",
    "    \"\"\"\n",
    "    Compute the average SSIM between the original and reconstructed datasets.\n",
    "    \n",
    "    Args:\n",
    "        original: numpy array\n",
    "            The original dataset.\n",
    "        reconstructed: numpy array\n",
    "            The reconstructed dataset.\n",
    "    \n",
    "    Returns:\n",
    "        avg_ssim: float\n",
    "            The average SSIM value.\n",
    "    \"\"\"\n",
    "    ssim_values = [ssim(original[i].reshape(8, 8), reconstructed[i].reshape(8, 8), data_range=16) for i in range(original.shape[0])]\n",
    "    return np.mean(ssim_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset and dictionary\n",
    "digits = load_digits()\n",
    "images = digits.data\n",
    "dictionary, dict_learner = learn_dictionary_from_digits()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test OMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average SSIM: 0.9530500921899188\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reconstruct the dataset using OMP\n",
    "reconstructed_images = reconstruct_with_OMP(dictionary, images, dict_learner)\n",
    "\n",
    "# Evaluate the average SSIM\n",
    "avg_ssim_value = average_ssim(images, reconstructed_images)\n",
    "print(f\"Average SSIM: {avg_ssim_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the dataset and dictionary\n",
    "digits = load_digits()\n",
    "images = digits.data\n",
    "dictionary, dict_learner = learn_dictionary_from_digits()\n",
    "# Define the RL environment\n",
    "\n",
    "class OMPEnvironment(gym.Env):\n",
    "    \"\"\"\n",
    "    A Reinforcement Learning environment that implements Orthogonal Matching Pursuit (OMP) algorithm for sparse coding.\n",
    "    \n",
    "    Attributes:\n",
    "        dictionary (np.array): The dictionary matrix for sparse coding.\n",
    "        image (np.array): The input image data to be reconstructed.\n",
    "        num_atoms (int): The number of atoms in the dictionary.\n",
    "        observation_space (gym.spaces.Box): The observation space representing the selected atoms.\n",
    "        action_space (gym.spaces.Discrete): The action space (0 for OMP, 1 for alternative strategy).\n",
    "        residual (np.array): The current residual of the input image.\n",
    "        selected_indices (list): The indices of the selected atoms from the dictionary.\n",
    "        approximated_signals (list): The list of approximated signals at each step.\n",
    "    \"\"\"\n",
    "    def __init__(self, dictionary, y):\n",
    "        self.dictionary = dictionary\n",
    "        self.y = y\n",
    "        self.num_atoms = dictionary.shape[1]\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(self.num_atoms,))\n",
    "        self.residual = y\n",
    "        self.approximated_signals = []\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.seed()\n",
    "        self.reset()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.residual = self.t.copy()\n",
    "        self.selected_indices = []\n",
    "        self.approximated_signals = []\n",
    "        return np.zeros(self.num_atoms)\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0 or not self.selected_indices:  # OMP action or first atom selection\n",
    "            correlations = np.abs(np.dot(self.dictionary.T, self.residual))\n",
    "            for index in self.selected_indices:\n",
    "                correlations[index] = 0  # Zero out already selected atoms\n",
    "            selected_atom_index = np.argmax(correlations)\n",
    "            self.selected_indices.append(selected_atom_index)\n",
    "\n",
    "            # Create sub-dictionary with selected atoms\n",
    "            selected_atoms = self.dictionary[:, self.selected_indices]\n",
    "\n",
    "            # Solve the least squares problem to find the best coefficients\n",
    "            coefficients, _, _, _ = np.linalg.lstsq(selected_atoms, self.residual, rcond=None)\n",
    "\n",
    "            # Reconstruct the signal using the coefficients\n",
    "            approximated_signal = selected_atoms.dot(coefficients)\n",
    "\n",
    "        elif action == 1:  # New decision strategy\n",
    "            # Find the atom least correlated with the selected atoms\n",
    "            selected_atoms = self.dictionary[:, self.selected_indices]\n",
    "            correlations = np.abs(selected_atoms.T @ self.dictionary)\n",
    "            avg_correlations = np.mean(correlations, axis=0)\n",
    "            avg_correlations[self.selected_indices] = np.inf  # Exclude already selected\n",
    "            selected_atom_index = np.argmin(avg_correlations)\n",
    "\n",
    "            # Update selected indices\n",
    "            self.selected_indices.append(selected_atom_index)\n",
    "\n",
    "            # Solve the least squares problem to find the best coefficients\n",
    "            coefficients, _, _, _ = np.linalg.lstsq(selected_atoms, self.residual, rcond=None)\n",
    "\n",
    "            # Reconstruct the signal using the coefficients\n",
    "            approximated_signal = selected_atoms.dot(coefficients)\n",
    "\n",
    "\n",
    "        # Update residual and calculate reward\n",
    "        self.residual = self.residual - approximated_signal.flatten()\n",
    "        self.approximated_signals.append(approximated_signal.flatten())\n",
    "        done = np.allclose(self.residual, np.zeros_like(self.residual), atol=1e-3)\n",
    "        reward = -np.linalg.norm(self.residual)\n",
    "\n",
    "        # Prepare the observation\n",
    "        observation = np.zeros(self.num_atoms)\n",
    "        observation[self.selected_indices] = 1\n",
    "\n",
    "        return observation, reward, done, {}\n",
    "# Create the environment with an image\n",
    "env = OMPEnvironment(dictionary, images[0])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (100,) and (64,) not aligned: 100 (dim 0) != 64 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Fin Amin\\Desktop\\DSSP\\neural_matched_pursuit.ipynb Cell 11\u001b[0m line \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/DSSP/neural_matched_pursuit.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m env \u001b[39m=\u001b[39m OMPEnvironment(dictionary, images[\u001b[39m0\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/DSSP/neural_matched_pursuit.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m agent \u001b[39m=\u001b[39m build_agent(env)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/DSSP/neural_matched_pursuit.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m agent\u001b[39m.\u001b[39;49mfit(env, nb_steps\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m, visualize\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, verbose\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/DSSP/neural_matched_pursuit.ipynb#X13sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Modify the OMP function to use the RL agent\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/DSSP/neural_matched_pursuit.ipynb#X13sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39morthogonal_matching_pursuit_rl\u001b[39m(dictionary, target_signal, sparsity, agent):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\rl\\core.py:176\u001b[0m, in \u001b[0;36mAgent.fit\u001b[1;34m(self, env, nb_steps, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, start_step_policy, log_interval, nb_max_episode_steps)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(action_repetition):\n\u001b[0;32m    175\u001b[0m     callbacks\u001b[39m.\u001b[39mon_action_begin(action)\n\u001b[1;32m--> 176\u001b[0m     observation, r, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m    177\u001b[0m     observation \u001b[39m=\u001b[39m deepcopy(observation)\n\u001b[0;32m    178\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;32mc:\\Users\\Fin Amin\\Desktop\\DSSP\\neural_matched_pursuit.ipynb Cell 11\u001b[0m line \u001b[0;36mOMPEnvironment.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/DSSP/neural_matched_pursuit.ipynb#X13sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m selected_atom \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdictionary[:, action]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/DSSP/neural_matched_pursuit.ipynb#X13sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m selected_atom_norm \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(selected_atom)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/DSSP/neural_matched_pursuit.ipynb#X13sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m approximated_signal \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(selected_atom\u001b[39m.\u001b[39;49mT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimage) \u001b[39m/\u001b[39m selected_atom_norm\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/DSSP/neural_matched_pursuit.ipynb#X13sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m residual \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39mdot(selected_atom, approximated_signal)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/DSSP/neural_matched_pursuit.ipynb#X13sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m done \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mallclose(residual, np\u001b[39m.\u001b[39mzeros_like(residual))\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (100,) and (64,) not aligned: 100 (dim 0) != 64 (dim 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the RL agent\n",
    "def build_agent(env):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(env.action_space.n))\n",
    "    model.add(Activation('linear'))\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    policy = BoltzmannQPolicy()\n",
    "    dqn = DQNAgent(model=model, nb_actions=env.action_space.n, memory=memory, nb_steps_warmup=10,\n",
    "                   target_model_update=1e-2, policy=policy)\n",
    "    dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "    return dqn\n",
    "\n",
    "# Train the RL agent\n",
    "env = OMPEnvironment(dictionary, images[0])\n",
    "agent = build_agent(env)\n",
    "agent.fit(env, nb_steps=10000, visualize=False, verbose=2)\n",
    "\n",
    "# Modify the OMP function to use the RL agent\n",
    "def orthogonal_matching_pursuit_rl(dictionary, target_signal, sparsity, agent):\n",
    "    num_atoms = dictionary.shape[1]\n",
    "    selected_indices = []\n",
    "    approximated_signal = np.zeros(num_atoms)\n",
    "    residual = target_signal\n",
    "\n",
    "    for _ in range(sparsity):\n",
    "        observation = np.zeros(num_atoms)\n",
    "        observation[selected_indices] = 1\n",
    "        action = agent.forward(observation)\n",
    "        correlations = np.abs(np.dot(dictionary.T, residual))\n",
    "        correlations[selected_indices] = 0  # Exclude already selected indices\n",
    "        # selected_index = np.argmax(correlations)\n",
    "        selected_index = action\n",
    "        selected_indices.append(selected_index)\n",
    "\n",
    "        selected_atom = dictionary[:, selected_index]\n",
    "        selected_atom_norm = np.linalg.norm(selected_atom)**2\n",
    "        approximated_signal[selected_index] += np.dot(selected_atom.T, target_signal) / selected_atom_norm\n",
    "\n",
    "        residual = target_signal - np.dot(dictionary[:, np.array(selected_indices)], approximated_signal[np.array(selected_indices)])\n",
    "\n",
    "    return approximated_signal, selected_indices\n",
    "\n",
    "# Perform OMP with RL on the dataset\n",
    "sparsity = 50  # Desired sparsity level\n",
    "ssim_scores = []\n",
    "for image in tqdm(images):\n",
    "    approximated_image, selected_indices = orthogonal_matching_pursuit_rl(dict_learner.components_.T, image, sparsity, agent)\n",
    "    reconstructed_image = np.dot(images.T[:, selected_indices], approximated_image[selected_indices])\n",
    "    ssim_score = ssim(image.reshape(8, 8), reconstructed_image.reshape(8, 8), data_range=16)\n",
    "    ssim_scores.append(ssim_score)\n",
    "\n",
    "# Calculate average SSIM\n",
    "average_ssim = np.mean(ssim_scores)\n",
    "\n",
    "print(\"Average SSIM:\", average_ssim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def orthogonal_matching_pursuit(dictionary, target_signal, sparsity):\n",
    "    num_atoms = dictionary.shape[1]\n",
    "    selected_indices = []\n",
    "    approximated_signal = np.zeros(num_atoms)\n",
    "    residual = target_signal\n",
    "\n",
    "    for _ in range(sparsity):\n",
    "        correlations = np.abs(np.dot(dictionary.T, residual))\n",
    "        correlations[selected_indices] = 0  # Exclude already selected indices\n",
    "        selected_index = np.argmax(correlations)\n",
    "        selected_indices.append(selected_index)\n",
    "\n",
    "        selected_atom = dictionary[:, selected_index]\n",
    "        selected_atom_norm = np.linalg.norm(selected_atom)**2\n",
    "        approximated_signal[selected_index] += np.dot(selected_atom.T, target_signal) / selected_atom_norm\n",
    "\n",
    "        residual = target_signal - np.dot(dictionary[:, np.array(selected_indices)], approximated_signal[np.array(selected_indices)])\n",
    "\n",
    "    return approximated_signal, selected_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load MNIST dataset\n",
    "digits = load_digits()  # Replace with appropriate code to load MNIST dataset\n",
    "images = digits.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f94fe0de110>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[6].reshape(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_learner = DictionaryLearning(\n",
    "    n_components=64,\n",
    "    fit_algorithm='cd',\n",
    "    transform_algorithm= 'omp', \n",
    "    random_state=0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DictionaryLearning(fit_algorithm='cd', n_components=64, random_state=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_learner.fit(images[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1797/1797 [00:01<00:00, 1296.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average SSIM: 2.9224656938283083e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ssim_scores = []\n",
    "\n",
    "for image in tqdm(images):\n",
    "    reconstructed_image = dict_learner.transform(image.reshape(1,-1)) \n",
    "    image = image.reshape(8,8) \n",
    "    reconstructed_image = reconstructed_image.reshape(8,8) \n",
    "    ssim_score = ssim(image, reconstructed_image, data_range=reconstructed_image.max() - reconstructed_image.min())\n",
    "    ssim_scores.append(ssim_score)\n",
    "\n",
    "# Calculate average SSIM\n",
    "average_ssim = np.mean(ssim_scores)\n",
    "\n",
    "print(\"Average SSIM:\", average_ssim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x19be48671c0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(reconstructed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1797/1797 [00:03<00:00, 585.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average SSIM: 0.00012126993999819036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# OMP parameters\n",
    "sparsity = 50  # Desired sparsity level\n",
    "\n",
    "# Perform OMP on each image\n",
    "ssim_scores = []\n",
    "for image in tqdm(images):\n",
    "    approximated_image, selected_indices = orthogonal_matching_pursuit(dict_learner.components_.T, image, sparsity)\n",
    "    reconstructed_image = np.dot(images.T[:, selected_indices], approximated_image[selected_indices])\n",
    "    reconstructed_image = reconstructed_image.reshape(8,8) \n",
    "    image = image.reshape(8,8) \n",
    "    ssim_score = ssim(image, reconstructed_image, data_range=reconstructed_image.max() - reconstructed_image.min())\n",
    "    ssim_scores.append(ssim_score)\n",
    "\n",
    "# Calculate average SSIM\n",
    "average_ssim = np.mean(ssim_scores)\n",
    "\n",
    "print(\"Average SSIM:\", average_ssim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [2],\n",
       "       ...,\n",
       "       [8],\n",
       "       [9],\n",
       "       [8]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,  -434,  1140, ...,  1764, -2952,  -520],\n",
       "       [    0,   290,  -759, ..., -1166,  1976,   348],\n",
       "       [    0,   126,  -329, ...,  -493,   864,   151],\n",
       "       ...,\n",
       "       [    0,   171,  -445, ...,  -686,  1161,   205],\n",
       "       [    0,   -86,   228, ...,   361,  -589,  -104],\n",
       "       [    0,  2671,  6151, ...,  5266,  1101,  -207]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
