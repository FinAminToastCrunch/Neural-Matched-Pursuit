{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
    "from sklearn.decomposition import MiniBatchDictionaryLearning\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress deprecation warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check if TensorFlow can access the GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"GPU is NOT available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 64)\n"
     ]
    }
   ],
   "source": [
    "def learn_dictionary_from_digits():\n",
    "    \"\"\"\n",
    "    Load the digits dataset and learn an over-determined dictionary from it using MiniBatchDictionaryLearning.\n",
    "    \n",
    "    Returns:\n",
    "        dictionary: numpy array\n",
    "            The learned dictionary.\n",
    "        dict_learner: MiniBatchDictionaryLearning object\n",
    "            The trained dictionary learner.\n",
    "    \"\"\"\n",
    "    # Load the digits dataset\n",
    "    digits = load_digits()\n",
    "    images = digits.data\n",
    "    \n",
    "    # Set the number of components (atoms) for the dictionary to ensure it's over-determined\n",
    "    n_components = 100  # Greater than 64 (the dimensionality of the input data)\n",
    "    \n",
    "    # Use MiniBatchDictionaryLearning to learn a dictionary\n",
    "    dict_learner = MiniBatchDictionaryLearning(n_components=n_components, alpha=1, random_state=0, n_jobs=-1, transform_algorithm='omp',)\n",
    "    dictionary = dict_learner.fit(images).components_\n",
    "    \n",
    "    return dictionary, dict_learner\n",
    "\n",
    "# Execute the function to get the learned dictionary and the dictionary learner object\n",
    "dictionary, dict_learner = learn_dictionary_from_digits()\n",
    "print(dictionary.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_with_OMP(dictionary, images, dict_learner):\n",
    "    \"\"\"\n",
    "    Reconstruct the dataset using MiniBatchDictionaryLearning's transform method and a given dictionary.\n",
    "    \n",
    "    Args:\n",
    "        dictionary: numpy array\n",
    "            The learned dictionary.\n",
    "        images: numpy array\n",
    "            The dataset to be reconstructed.\n",
    "        dict_learner: MiniBatchDictionaryLearning object\n",
    "            The trained dictionary learner.\n",
    "    \n",
    "    Returns:\n",
    "        reconstructed: numpy array\n",
    "            The reconstructed dataset.\n",
    "    \"\"\"\n",
    "    # Get the sparse representation of the images using the dictionary learner\n",
    "    coefficients = dict_learner.transform(images)\n",
    "\n",
    "    # print(coefficients.shape)\n",
    "    dictionary = dictionary\n",
    "    \n",
    "    # Reconstruct the images from the coefficients and the dictionary\n",
    "    reconstructed = np.dot(coefficients, dictionary)\n",
    "    return reconstructed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def average_ssim(original, reconstructed):\n",
    "    \"\"\"\n",
    "    Compute the average SSIM between the original and reconstructed datasets.\n",
    "    \n",
    "    Args:\n",
    "        original: numpy array\n",
    "            The original dataset.\n",
    "        reconstructed: numpy array\n",
    "            The reconstructed dataset.\n",
    "    \n",
    "    Returns:\n",
    "        avg_ssim: float\n",
    "            The average SSIM value.\n",
    "    \"\"\"\n",
    "    ssim_values = [ssim(original[i].reshape(8, 8), reconstructed[i].reshape(8, 8), data_range=16) for i in range(original.shape[0])]\n",
    "    return np.mean(ssim_values)\n",
    "\n",
    "\n",
    "def average_mae(original, reconstructed):\n",
    "    \"\"\"\n",
    "    Compute the average Mean Absolute Error (MAE) between the original and reconstructed datasets.\n",
    "    \n",
    "    Args:\n",
    "        original: numpy array\n",
    "            The original dataset.\n",
    "        reconstructed: numpy array\n",
    "            The reconstructed dataset.\n",
    "    \n",
    "    Returns:\n",
    "        avg_mae: float\n",
    "            The average MAE value.\n",
    "    \"\"\"\n",
    "    mae_values = [np.mean(np.abs(original[i] - reconstructed[i])) for i in range(original.shape[0])]\n",
    "    return np.mean(mae_values)\n",
    "\n",
    "\n",
    "def average_2norm(original, reconstructed):\n",
    "    \"\"\"\n",
    "    Compute the average 2-norm (Euclidean norm) between the original and reconstructed datasets.\n",
    "    \n",
    "    Args:\n",
    "        original: numpy array\n",
    "            The original dataset.\n",
    "        reconstructed: numpy array\n",
    "            The reconstructed dataset.\n",
    "    \n",
    "    Returns:\n",
    "        avg_2norm: float\n",
    "            The average 2-norm value.\n",
    "    \"\"\"\n",
    "    norm_values = [np.linalg.norm(original[i] - reconstructed[i]) for i in range(original.shape[0])]\n",
    "    return np.mean(norm_values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset and dictionary\n",
    "digits = load_digits()\n",
    "images = digits.data\n",
    "dictionary, dict_learner = learn_dictionary_from_digits()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test OMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average SSIM: 0.9530500921899188\n",
      "Average MAE: 1.1376603906458562\n",
      "Average 2-norm: 13.189506487490748\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reconstruct the dataset using OMP\n",
    "reconstructed_images = reconstruct_with_OMP(dictionary, images, dict_learner)\n",
    "\n",
    "# Evaluate the average SSIM\n",
    "avg_ssim_value = average_ssim(images, reconstructed_images)\n",
    "print(f\"Average SSIM: {avg_ssim_value}\")\n",
    "\n",
    "avg_mae_value = average_mae(images, reconstructed_images)\n",
    "print(f\"Average MAE: {avg_mae_value}\")\n",
    "\n",
    "average_2_norm = average_2norm(images, reconstructed_images)\n",
    "print(f\"Average 2-norm: {average_2_norm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "print(dict_learner.n_features_in_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "reshaped_images = reconstructed_images[-4:].reshape(-1, 8, 8)\n",
    "\n",
    "# Display the images using plt\n",
    "fig, axs = plt.subplots(1, 4, figsize=(10, 5))\n",
    "for i in range(4):\n",
    "    axs[i].imshow(reshaped_images[i], cmap='gray')\n",
    "    axs[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "reshaped_images = images[-4:].reshape(-1, 8, 8)\n",
    "\n",
    "# Display the images using plt\n",
    "fig, axs = plt.subplots(1, 4, figsize=(10, 5))\n",
    "for i in range(4):\n",
    "    axs[i].imshow(reshaped_images[i], cmap='gray')\n",
    "    axs[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMP\n",
    "\n",
    "Note, for now we are using the images dataset as the training and test set. We should split these up later?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the dataset and dictionary\n",
    "digits = load_digits()\n",
    "images = digits.data\n",
    "dictionary, dict_learner = learn_dictionary_from_digits()\n",
    "# Define the RL environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 104s 10ms/step - reward: -42.3460\n",
      "312 episodes - episode_reward: -1355.166 [-1564.002, -1081.367] - loss: 9583.333 - mae: 360.146 - mean_q: -627.530\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 113s 11ms/step - reward: -42.3915\n",
      "done, took 217.855 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b03c8e16d0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class OMPEnvironment(gym.Env):\n",
    "    \"\"\"\n",
    "    A Reinforcement Learning environment that implements Orthogonal Matching Pursuit (OMP) algorithm for sparse coding.\n",
    "    \n",
    "    Attributes:\n",
    "        dictionary (np.array): The dictionary matrix for sparse coding.\n",
    "        image (np.array): The input image data to be reconstructed.\n",
    "        num_atoms (int): The number of atoms in the dictionary.\n",
    "        observation_space (gym.spaces.Box): The observation space representing the selected atoms.\n",
    "        action_space (gym.spaces.Discrete): The action space (0 for OMP, 1 for alternative strategy).\n",
    "        residual (np.array): The current residual of the input image.\n",
    "        selected_indices (list): The indices of the selected atoms from the dictionary.\n",
    "        approximated_signals (list): The list of approximated signals at each step.\n",
    "    \"\"\"\n",
    "    def __init__(self, dictionary, yFull, non_zero_coeff=None):\n",
    "        self.dictionary = dictionary.T\n",
    "        self.yFull = yFull\n",
    "        self.i = 0\n",
    "        self.y = yFull[self.i] \n",
    "        self.num_atoms = dictionary.shape[0]\n",
    "        self.num_feats = dictionary.shape[1]\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.residual = self.y.copy()\n",
    "        self.approximated_signals = []\n",
    "        if non_zero_coeff is None:\n",
    "            self.non_zero_coeff = int(self.num_feats/10)\n",
    "        else:\n",
    "            self.non_zero_coeff = non_zero_coeff\n",
    "\n",
    "        # Define the low and high bounds of each observation component\n",
    "        # Residual Norm can theoretically go up to infinity, but we set a reasonable upper bound based on the input data\n",
    "        residual_norm_bounds = (0, np.linalg.norm(self.y))\n",
    "        # Determinant of the Gram Matrix can range from 0 to potentially a large number, but depends on the dictionary\n",
    "        det_gram_matrix_bounds = (0, np.inf)\n",
    "        # Mutual coherence ranges between 0 (no coherence) and 1 (perfect coherence)\n",
    "        mutual_coherence_bounds = (0, 1)\n",
    "\n",
    "        # Observation space now consists of the four components defined above\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([residual_norm_bounds[0], det_gram_matrix_bounds[0], mutual_coherence_bounds[0], mutual_coherence_bounds[0]]),\n",
    "            high=np.array([residual_norm_bounds[1], det_gram_matrix_bounds[1], mutual_coherence_bounds[1], mutual_coherence_bounds[1]]),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        self.seed()\n",
    "        self.reset()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.i = (self.i + 1) % self.yFull.shape[0]\n",
    "        self.y = self.yFull[self.i] \n",
    "        self.residual = self.y.copy()\n",
    "        self.selected_indices = []\n",
    "        ob = self.get_obs()\n",
    "        return ob\n",
    "    \n",
    "    def get_obs(self):\n",
    "        # 1) Residual Norm\n",
    "        residual_norm = np.linalg.norm(self.residual)\n",
    "\n",
    "        # 2) Determinant of the Gram Matrix for already selected atoms\n",
    "        if self.selected_indices:\n",
    "            selected_atoms = self.dictionary[:, self.selected_indices]\n",
    "            gram_matrix = np.dot(selected_atoms.T, selected_atoms)\n",
    "            det_gram_matrix = np.linalg.det(gram_matrix)\n",
    "        else:\n",
    "            det_gram_matrix = 0  # Placeholder when no atoms are selected\n",
    "\n",
    "        # 3) Mutual coherence of the selected atoms\n",
    "        if self.selected_indices:\n",
    "            normalized_gram_matrix = gram_matrix / np.outer(\n",
    "                np.linalg.norm(selected_atoms, axis=0),\n",
    "                np.linalg.norm(selected_atoms, axis=0)\n",
    "            )\n",
    "            mutual_coherence_selected = np.max(np.abs(normalized_gram_matrix - np.eye(len(self.selected_indices))))\n",
    "        else:\n",
    "            mutual_coherence_selected = 0  # Placeholder when no atoms are selected\n",
    "\n",
    "        # 4) Mutual coherence of the unselected atoms\n",
    "        if len(self.selected_indices) < self.num_atoms:\n",
    "            unselected_atoms = self.dictionary[:, [i for i in range(self.num_atoms) if i not in self.selected_indices]]\n",
    "            unselected_gram_matrix = np.dot(unselected_atoms.T, unselected_atoms)\n",
    "            normalized_unselected_gram_matrix = unselected_gram_matrix / np.outer(\n",
    "                np.linalg.norm(unselected_atoms, axis=0),\n",
    "                np.linalg.norm(unselected_atoms, axis=0)\n",
    "            )\n",
    "            mutual_coherence_unselected = np.max(np.abs(normalized_unselected_gram_matrix - np.eye(unselected_atoms.shape[1])))\n",
    "        else:\n",
    "            mutual_coherence_unselected = 0  # Placeholder when no atoms are left to select\n",
    "\n",
    "        # Concatenate all observations into a single numpy array\n",
    "        c =  np.array([\n",
    "            residual_norm,\n",
    "            det_gram_matrix,\n",
    "            mutual_coherence_selected,\n",
    "            mutual_coherence_unselected\n",
    "        ])\n",
    "\n",
    "        # print(c.shape)\n",
    "        return c\n",
    "    \n",
    "    def step(self, action):\n",
    "\n",
    "        # action = 0 # uncomment this to have algo mimic OMP\n",
    "\n",
    "        if action == 0 or not self.selected_indices:  # OMP action or first atom selection\n",
    "            correlations = np.abs(np.dot(self.dictionary.T, self.residual))\n",
    "            for index in self.selected_indices:\n",
    "                correlations[index] = 0  # Zero out already selected atoms\n",
    "            selected_atom_index = np.argmax(correlations)\n",
    "            self.selected_indices.append(selected_atom_index)\n",
    "\n",
    "            # Create sub-dictionary with selected atoms\n",
    "            selected_atoms = self.dictionary[:, self.selected_indices]\n",
    "\n",
    "            # Solve the least squares problem to find the best coefficients\n",
    "            coefficients, _, _, _ = np.linalg.lstsq(selected_atoms, self.residual, rcond=None)\n",
    "\n",
    "            # # Reconstruct the signal using the coefficients\n",
    "            # approximated_signal = selected_atoms.dot(coefficients)\n",
    "\n",
    "        elif action == 1:  # New decision strategy\n",
    "            # Find the atom least correlated with the selected atoms\n",
    "            selected_atoms = self.dictionary[:, self.selected_indices]\n",
    "            correlations = np.abs(selected_atoms.T @ self.dictionary)\n",
    "            avg_correlations = np.mean(correlations, axis=0)\n",
    "            avg_correlations[self.selected_indices] = np.inf  # Exclude already selected\n",
    "            selected_atom_index = np.argmin(avg_correlations)\n",
    "\n",
    "            # Update selected indices\n",
    "            self.selected_indices.append(selected_atom_index)\n",
    "            selected_atoms = self.dictionary[:, self.selected_indices]\n",
    "\n",
    "            # Solve the least squares problem to find the best coefficients\n",
    "            coefficients, _, _, _ = np.linalg.lstsq(selected_atoms, self.residual, rcond=None)\n",
    "\n",
    "            # # Reconstruct the signal using the coefficients\n",
    "            # approximated_signal = selected_atoms.dot(coefficients)\n",
    "        # Reconstruct the signal using the coefficients from least squares\n",
    "        approximated_signal = np.dot(selected_atoms, coefficients)\n",
    "\n",
    "        # Update residual as the difference between the original signal and the approximated signal\n",
    "        self.residual = self.y - approximated_signal\n",
    "\n",
    "        # Add the approximated signal to the history\n",
    "        self.approximated_signals.append(approximated_signal)\n",
    "\n",
    "        # Check if the reconstruction is sufficiently accurate\n",
    "        done = False #np.allclose(self.y, approximated_signal, atol=1e-3) #same tolerance as OMP\n",
    "\n",
    "        if(len(self.selected_indices) == int(self.num_feats/2)):\n",
    "            done = True\n",
    "\n",
    "        # Calculate the reward as the negative norm of the residual\n",
    "        reward = -np.linalg.norm(self.residual)\n",
    "\n",
    "        observation = self.get_obs() \n",
    "\n",
    "        return observation, reward, done, {}\n",
    "    \n",
    "# Create the environment with an image\n",
    "env = OMPEnvironment(dictionary, images)\n",
    "    \n",
    "def build_agent(env):\n",
    "    \"\"\"\n",
    "    Builds a Deep Q-Network (DQN) agent for the given environment with a more complex model.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment for which the agent is built.\n",
    "\n",
    "    Returns:\n",
    "        DQNAgent: An instance of the DQN agent.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    # Adjust the input shape to match the observation space of the environment\n",
    "    model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(env.action_space.n, activation='linear'))\n",
    "\n",
    "    memory = SequentialMemory(limit=20000, window_length=1)\n",
    "    policy = BoltzmannQPolicy()\n",
    "    dqn = DQNAgent(model=model, \n",
    "                   nb_actions=env.action_space.n, \n",
    "                   memory=memory, \n",
    "                   nb_steps_warmup=100,\n",
    "                   target_model_update=10, \n",
    "                   policy=policy)\n",
    "\n",
    "    dqn.compile(Adam(lr=1e-4), metrics=['mae'])\n",
    "\n",
    "    return dqn\n",
    "\n",
    "# Assume 'dictionary' and 'images' are defined and initialized elsewhere in the code\n",
    "env = OMPEnvironment(dictionary, images, non_zero_coeff=32)\n",
    "agent = build_agent(env)\n",
    "agent.fit(env, nb_steps=20000, visualize=False, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(env.approximated_signals)\n",
    "ap = np.array(env.approximated_signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average SSIM: 0.19848981246357672\n",
      "Average MAE: 4.51786140391659\n",
      "Average 2-norm: 54.41155926617995\n"
     ]
    }
   ],
   "source": [
    "reconstructed_images = ap\n",
    "\n",
    "avg_ssim_value = average_ssim(images, reconstructed_images)\n",
    "print(f\"Average SSIM: {avg_ssim_value}\")\n",
    "\n",
    "avg_mae_value = average_mae(images, reconstructed_images)\n",
    "print(f\"Average MAE: {avg_mae_value}\")\n",
    "\n",
    "average_2norm = average_2norm(images, reconstructed_images)\n",
    "print(f\"Average 2-norm: {average_2norm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reshape the last 4 images to 8 by 8\n",
    "reshaped_images = ap[-4:].reshape(-1, 8, 8)\n",
    "\n",
    "# Display the images using plt\n",
    "fig, axs = plt.subplots(1, 4, figsize=(10, 5))\n",
    "for i in range(4):\n",
    "    axs[i].imshow(reshaped_images[i], cmap='gray')\n",
    "    axs[i].axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAACHCAYAAAASnYMFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAFtklEQVR4nO3b0XHaWBiG4cOO71EqQB2YDlAJdBBSQUwFwRWEVGDSAVSQuAKTCgwVBCrQXuzlOvHmPzDy/Ps8t55PUuyzmnc0s6O+7wsAQGZ/Df0AAADXJngAgPQEDwCQnuABANITPABAeje/++FoNBrkf+FaLBZV+4eHh/D2x48f4e3hcAhva//Np9MpvO37flR181cMdY6apqnabzab8LbruvC2bdvwtuYc1LrmORrqDNX8LUopZbVahbf7/T68Xa/X4e2Qsr6Lat4lpdS9T2ruXXOO3uK7yBceACA9wQMApCd4AID0BA8AkJ7gAQDSEzwAQHqCBwBIT/AAAOkJHgAgPcEDAKQneACA9AQPAJCe4AEA0hM8AEB6o77vf/3D0ejXP7yi3z3Tf3E+n8Pb/X4f3s5ms/D23bt34W0ppZxOp/C27/tR1c1fMdQ5qvlbllLK7e1teHt/fx/ebjab8LbmHNTur3mOhjpDh8Ohaj+ZTC7zIH+o5h3Ytm3Vvd/qGSql7hzV/F6en5/D21JKOR6P4e1qtQpvt9tteFv7Lqrxq3PkCw8AkJ7gAQDSEzwAQHqCBwBIT/AAAOkJHgAgPcEDAKQneACA9AQPAJCe4AEA0hM8AEB6ggcASE/wAADpCR4AIL2ba114Op1e69KvWq1W4e16vQ5v9/t9eDufz8PbUkrZbDZV+4xub2+r9ufzObxt2za8fX5+Dm9rz9Fut6vav0Vd14W3k8mk6t7L5TK8/f79e3j79PQU3i4Wi/C2lLp36Ft2OBzC25p3SSmlNE0T3ta8i2r+zTXPfC2+8AAA6QkeACA9wQMApCd4AID0BA8AkJ7gAQDSEzwAQHqCBwBIT/AAAOkJHgAgPcEDAKQneACA9AQPAJCe4AEA0hM8AEB6N9e6cNM017r0q9br9SD33e/34W3bthd7Dv5xf39ftf/06VN4+/79+/B2uVyGt7vdLrzNajweD3bv6XQ62L2jat5jvKzmfVBKKdvtNryteY99/fo1vH2LfOEBANITPABAeoIHAEhP8AAA6QkeACA9wQMApCd4AID0BA8AkJ7gAQDSEzwAQHqCBwBIT/AAAOkJHgAgPcEDAKR3c60Ld113rUu/WTX/5tVqdbHn4B9N0wz9CCH7/X7oR0hlt9uFt/P5vOreX758CW//j+/QrJbLZdX+fD5f6En+TNu2g9z3WnzhAQDSEzwAQHqCBwBIT/AAAOkJHgAgPcEDAKQneACA9AQPAJCe4AEA0hM8AEB6ggcASE/wAADpCR4AID3BAwCkd3OtCz89PV3r0q+6u7sLb9u2DW8nk0l4+/Pnz/CWl338+LFqfzwew9uas7DdbsPbpmnCW/5tt9sNuo/q+z68PRwOl3uQRLquC29ns1nVvT98+BDe1vw9v337Ft4uFovwtpRSNptN1f4lvvAAAOkJHgAgPcEDAKQneACA9AQPAJCe4AEA0hM8AEB6ggcASE/wAADpCR4AID3BAwCkJ3gAgPQEDwCQnuABANK7udaFHx8fw9vz+Vx178+fP1fth1Dz++JlteeoaZpB7j0ej8NbLqvruqr9dDq9yHMwvNqzMNS9D4fDxZ7jT7RtO8h9f8cXHgAgPcEDAKQneACA9AQPAJCe4AEA0hM8AEB6ggcASE/wAADpCR4AID3BAwCkJ3gAgPQEDwCQnuABANITPABAeoIHAEjv5loXPp1O4e3d3V3VvR8eHsLb4/EY3q5Wq/C25vfFy6bTadW+5hx2XRfertfr8JbLGo/HVfv5fB7ezmaz8Pbx8TG8PRwO4W1mQ/53WfM+qdnWnKO3+B7zhQcASE/wAADpCR4AID3BAwCkJ3gAgPQEDwCQnuABANITPABAeoIHAEhP8AAA6QkeACA9wQMApCd4AID0BA8AkN6o7/uhnwEA4Kp84QEA0hM8AEB6ggcASE/wAADpCR4AID3BAwCk9zfcGRjWuUawIQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 720x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "reshaped_images = images[-4:].reshape(-1, 8, 8)\n",
    "\n",
    "# Display the images using plt\n",
    "fig, axs = plt.subplots(1, 4, figsize=(10, 5))\n",
    "for i in range(4):\n",
    "    axs[i].imshow(reshaped_images[i], cmap='gray')\n",
    "    axs[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (100,) (64,) (100,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Fin Amin\\Desktop\\DSSP\\neural_matched_pursuit.ipynb Cell 16\u001b[0m line \u001b[0;36m<cell line: 52>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/DSSP/neural_matched_pursuit.ipynb#X42sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m ssim_scores \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/DSSP/neural_matched_pursuit.ipynb#X42sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images:  \u001b[39m# Assuming `images` is an iterable of 1D signals.\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/DSSP/neural_matched_pursuit.ipynb#X42sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     approximated_image, selected_indices \u001b[39m=\u001b[39m orthogonal_matching_pursuit_rl(dict_learner\u001b[39m.\u001b[39;49mcomponents_\u001b[39m.\u001b[39;49mT, image, sparsity, agent)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/DSSP/neural_matched_pursuit.ipynb#X42sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     reconstructed_image \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(dict_learner\u001b[39m.\u001b[39mcomponents_[:, selected_indices], approximated_image[selected_indices])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/DSSP/neural_matched_pursuit.ipynb#X42sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     ssim_score \u001b[39m=\u001b[39m ssim(image\u001b[39m.\u001b[39mreshape(\u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m), reconstructed_image\u001b[39m.\u001b[39mreshape(\u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m), data_range\u001b[39m=\u001b[39mimage\u001b[39m.\u001b[39mmax() \u001b[39m-\u001b[39m image\u001b[39m.\u001b[39mmin())\n",
      "\u001b[1;32mc:\\Users\\Fin Amin\\Desktop\\DSSP\\neural_matched_pursuit.ipynb Cell 16\u001b[0m line \u001b[0;36morthogonal_matching_pursuit_rl\u001b[1;34m(dictionary, target_signal, sparsity, agent)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/DSSP/neural_matched_pursuit.ipynb#X42sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     selected_atom \u001b[39m=\u001b[39m dictionary[:, action]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/DSSP/neural_matched_pursuit.ipynb#X42sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     coefficient \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(selected_atom\u001b[39m.\u001b[39mT, residual) \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(selected_atom)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/DSSP/neural_matched_pursuit.ipynb#X42sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     approximated_signal \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m coefficient \u001b[39m*\u001b[39m selected_atom\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/DSSP/neural_matched_pursuit.ipynb#X42sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     residual \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m coefficient \u001b[39m*\u001b[39m selected_atom\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/DSSP/neural_matched_pursuit.ipynb#X42sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mreturn\u001b[39;00m approximated_signal, selected_indices\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (100,) (64,) (100,) "
     ]
    }
   ],
   "source": [
    "\n",
    "def orthogonal_matching_pursuit_rl(dictionary, target_signal, sparsity, agent):\n",
    "    \"\"\"\n",
    "    Perform Orthogonal Matching Pursuit using a Reinforcement Learning agent.\n",
    "\n",
    "    Parameters:\n",
    "    dictionary : ndarray\n",
    "        The dictionary (or codebook) matrix with shape (n_features, n_components).\n",
    "    target_signal : ndarray\n",
    "        The signal to be approximated, with shape (n_features,).\n",
    "    sparsity : int\n",
    "        The desired level of sparsity (number of non-zero coefficients).\n",
    "    agent : RL Agent\n",
    "        The reinforcement learning agent to use for selecting atoms.\n",
    "\n",
    "    Returns:\n",
    "    approximated_signal : ndarray\n",
    "        The signal approximated using selected atoms from the dictionary.\n",
    "    selected_indices : list\n",
    "        The indices of the atoms selected from the dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    num_atoms = dictionary.shape[0]\n",
    "    selected_indices = []\n",
    "    approximated_signal = np.zeros((dictionary.shape[1],))\n",
    "    residual = target_signal.copy()\n",
    "\n",
    "    for _ in range(sparsity):\n",
    "        observation = np.zeros(num_atoms)\n",
    "        observation[selected_indices] = 1\n",
    "        \n",
    "        # This is where you'd get the action from the RL agent.\n",
    "        # For now, we will use a placeholder for the action, which would be\n",
    "        # the index with the highest correlation to the residual.\n",
    "        # action = agent.forward(observation)  # This should be correctly handled.\n",
    "        action = np.argmax(np.abs(np.dot(dictionary.T, residual)))\n",
    "\n",
    "        selected_indices.append(action)\n",
    "        selected_atom = dictionary[:, action]\n",
    "        coefficient = np.dot(selected_atom.T, residual) / np.linalg.norm(selected_atom)**2\n",
    "        approximated_signal += coefficient * selected_atom\n",
    "        residual -= coefficient * selected_atom\n",
    "\n",
    "    return approximated_signal, selected_indices\n",
    "\n",
    "# Initialize your RL agent here.\n",
    "# agent = YourRLAgent()\n",
    "\n",
    "# Perform OMP with RL on the dataset.\n",
    "sparsity = 50  # Desired sparsity level\n",
    "ssim_scores = []\n",
    "\n",
    "for image in images:  # Assuming `images` is an iterable of 1D signals.\n",
    "    approximated_image, selected_indices = orthogonal_matching_pursuit_rl(dict_learner.components_.T, image, sparsity, agent)\n",
    "    reconstructed_image = np.dot(dict_learner.components_[:, selected_indices], approximated_image[selected_indices])\n",
    "    ssim_score = ssim(image.reshape(8, 8), reconstructed_image.reshape(8, 8), data_range=image.max() - image.min())\n",
    "    ssim_scores.append(ssim_score)\n",
    "\n",
    "# Calculate average SSIM.\n",
    "average_ssim = np.mean(ssim_scores)\n",
    "\n",
    "print(\"Average SSIM:\", average_ssim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1797 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected flatten_21_input to have shape (1, 4) but got array with shape (1, 100)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Fin Amin\\Desktop\\DSSP\\neural_matched_pursuit.ipynb Cell 17\u001b[0m line \u001b[0;36m<cell line: 30>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/DSSP/neural_matched_pursuit.ipynb#X34sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m ssim_scores \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/DSSP/neural_matched_pursuit.ipynb#X34sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m tqdm(images):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/DSSP/neural_matched_pursuit.ipynb#X34sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     approximated_image, selected_indices \u001b[39m=\u001b[39m orthogonal_matching_pursuit_rl(dict_learner\u001b[39m.\u001b[39;49mcomponents_\u001b[39m.\u001b[39;49mT, image, sparsity, agent)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/DSSP/neural_matched_pursuit.ipynb#X34sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     reconstructed_image \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(images\u001b[39m.\u001b[39mT[:, selected_indices], approximated_image[selected_indices])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/DSSP/neural_matched_pursuit.ipynb#X34sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     ssim_score \u001b[39m=\u001b[39m ssim(image\u001b[39m.\u001b[39mreshape(\u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m), reconstructed_image\u001b[39m.\u001b[39mreshape(\u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m), data_range\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\Fin Amin\\Desktop\\DSSP\\neural_matched_pursuit.ipynb Cell 17\u001b[0m line \u001b[0;36morthogonal_matching_pursuit_rl\u001b[1;34m(dictionary, target_signal, sparsity, agent)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/DSSP/neural_matched_pursuit.ipynb#X34sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m observation \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(num_atoms)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/DSSP/neural_matched_pursuit.ipynb#X34sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m observation[selected_indices] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/DSSP/neural_matched_pursuit.ipynb#X34sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mforward(observation)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/DSSP/neural_matched_pursuit.ipynb#X34sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m correlations \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mabs(np\u001b[39m.\u001b[39mdot(dictionary\u001b[39m.\u001b[39mT, residual))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/DSSP/neural_matched_pursuit.ipynb#X34sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m correlations[selected_indices] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m  \u001b[39m# Exclude already selected indices\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\rl\\agents\\dqn.py:224\u001b[0m, in \u001b[0;36mDQNAgent.forward\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, observation):\n\u001b[0;32m    222\u001b[0m     \u001b[39m# Select an action.\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39mget_recent_state(observation)\n\u001b[1;32m--> 224\u001b[0m     q_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_q_values(state)\n\u001b[0;32m    225\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m    226\u001b[0m         action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39mselect_action(q_values\u001b[39m=\u001b[39mq_values)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\rl\\agents\\dqn.py:68\u001b[0m, in \u001b[0;36mAbstractDQNAgent.compute_q_values\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_q_values\u001b[39m(\u001b[39mself\u001b[39m, state):\n\u001b[1;32m---> 68\u001b[0m     q_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_batch_q_values([state])\u001b[39m.\u001b[39mflatten()\n\u001b[0;32m     69\u001b[0m     \u001b[39massert\u001b[39;00m q_values\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnb_actions,)\n\u001b[0;32m     70\u001b[0m     \u001b[39mreturn\u001b[39;00m q_values\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\rl\\agents\\dqn.py:63\u001b[0m, in \u001b[0;36mAbstractDQNAgent.compute_batch_q_values\u001b[1;34m(self, state_batch)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_batch_q_values\u001b[39m(\u001b[39mself\u001b[39m, state_batch):\n\u001b[0;32m     62\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_state_batch(state_batch)\n\u001b[1;32m---> 63\u001b[0m     q_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict_on_batch(batch)\n\u001b[0;32m     64\u001b[0m     \u001b[39massert\u001b[39;00m q_values\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (\u001b[39mlen\u001b[39m(state_batch), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnb_actions)\n\u001b[0;32m     65\u001b[0m     \u001b[39mreturn\u001b[39;00m q_values\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training_v1.py:1186\u001b[0m, in \u001b[0;36mModel.predict_on_batch\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1182\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   1183\u001b[0m       \u001b[39m'\u001b[39m\u001b[39m`predict_on_batch` is not supported for models distributed with\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1184\u001b[0m       \u001b[39m'\u001b[39m\u001b[39m tf.distribute.Strategy.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   1185\u001b[0m \u001b[39m# Validate and standardize user data.\u001b[39;00m\n\u001b[1;32m-> 1186\u001b[0m inputs, _, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_standardize_user_data(\n\u001b[0;32m   1187\u001b[0m     x, extract_tensors_from_dataset\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m   1188\u001b[0m \u001b[39m# If `self._distribution_strategy` is True, then we are in a replica context\u001b[39;00m\n\u001b[0;32m   1189\u001b[0m \u001b[39m# at this point.\u001b[39;00m\n\u001b[0;32m   1190\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_eagerly \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_distribution_strategy:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training_v1.py:2323\u001b[0m, in \u001b[0;36mModel._standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2319\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m run_eagerly \u001b[39mand\u001b[39;00m is_build_called \u001b[39mand\u001b[39;00m is_compile_called \u001b[39mand\u001b[39;00m\n\u001b[0;32m   2320\u001b[0m     \u001b[39mnot\u001b[39;00m is_dataset  \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(_is_symbolic_tensor(v) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m all_inputs)):\n\u001b[0;32m   2321\u001b[0m   \u001b[39mreturn\u001b[39;00m [], [], \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2323\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_standardize_tensors(\n\u001b[0;32m   2324\u001b[0m     x, y, sample_weight,\n\u001b[0;32m   2325\u001b[0m     run_eagerly\u001b[39m=\u001b[39;49mrun_eagerly,\n\u001b[0;32m   2326\u001b[0m     dict_inputs\u001b[39m=\u001b[39;49mdict_inputs,\n\u001b[0;32m   2327\u001b[0m     is_dataset\u001b[39m=\u001b[39;49mis_dataset,\n\u001b[0;32m   2328\u001b[0m     class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[0;32m   2329\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training_v1.py:2351\u001b[0m, in \u001b[0;36mModel._standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2348\u001b[0m \u001b[39m# Standardize the inputs.\u001b[39;00m\n\u001b[0;32m   2349\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(x, (tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset, tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset)):\n\u001b[0;32m   2350\u001b[0m   \u001b[39m# TODO(fchollet): run static checks with dataset output shape(s).\u001b[39;00m\n\u001b[1;32m-> 2351\u001b[0m   x \u001b[39m=\u001b[39m training_utils_v1\u001b[39m.\u001b[39;49mstandardize_input_data(\n\u001b[0;32m   2352\u001b[0m       x,\n\u001b[0;32m   2353\u001b[0m       feed_input_names,\n\u001b[0;32m   2354\u001b[0m       feed_input_shapes,\n\u001b[0;32m   2355\u001b[0m       check_batch_axis\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,  \u001b[39m# Don't enforce the batch size.\u001b[39;49;00m\n\u001b[0;32m   2356\u001b[0m       exception_prefix\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39minput\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m   2358\u001b[0m \u001b[39m# Get typespecs for the input data and sanitize it if necessary.\u001b[39;00m\n\u001b[0;32m   2359\u001b[0m \u001b[39m# TODO(momernick): This should be capable of doing full input validation\u001b[39;00m\n\u001b[0;32m   2360\u001b[0m \u001b[39m# at all times - validate that this is so and refactor the standardization\u001b[39;00m\n\u001b[0;32m   2361\u001b[0m \u001b[39m# code.\u001b[39;00m\n\u001b[0;32m   2362\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training_utils_v1.py:642\u001b[0m, in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    640\u001b[0m       \u001b[39mfor\u001b[39;00m dim, ref_dim \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(data_shape, shape):\n\u001b[0;32m    641\u001b[0m         \u001b[39mif\u001b[39;00m ref_dim \u001b[39m!=\u001b[39m dim \u001b[39mand\u001b[39;00m ref_dim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m dim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 642\u001b[0m           \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError when checking \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m exception_prefix \u001b[39m+\u001b[39m\n\u001b[0;32m    643\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39m: expected \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m names[i] \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m to have shape \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m\n\u001b[0;32m    644\u001b[0m                            \u001b[39mstr\u001b[39m(shape) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m but got array with shape \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m\n\u001b[0;32m    645\u001b[0m                            \u001b[39mstr\u001b[39m(data_shape))\n\u001b[0;32m    646\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected flatten_21_input to have shape (1, 4) but got array with shape (1, 100)"
     ]
    }
   ],
   "source": [
    "#I do not think this works\n",
    "# Modify the OMP function to use the RL agent\n",
    "def orthogonal_matching_pursuit_rl(dictionary, target_signal, sparsity, agent):\n",
    "    num_atoms = dictionary.shape[1]\n",
    "    selected_indices = []\n",
    "    approximated_signal = np.zeros(num_atoms)\n",
    "    residual = target_signal\n",
    "\n",
    "    for _ in range(sparsity):\n",
    "        observation = np.zeros(num_atoms)\n",
    "        observation[selected_indices] = 1\n",
    "        action = agent.forward(observation)\n",
    "        correlations = np.abs(np.dot(dictionary.T, residual))\n",
    "        correlations[selected_indices] = 0  # Exclude already selected indices\n",
    "        # selected_index = np.argmax(correlations)\n",
    "        selected_index = action\n",
    "        selected_indices.append(selected_index)\n",
    "\n",
    "        selected_atom = dictionary[:, selected_index]\n",
    "        selected_atom_norm = np.linalg.norm(selected_atom)**2\n",
    "        approximated_signal[selected_index] += np.dot(selected_atom.T, target_signal) / selected_atom_norm\n",
    "\n",
    "        residual = target_signal - np.dot(dictionary[:, np.array(selected_indices)], approximated_signal[np.array(selected_indices)])\n",
    "\n",
    "    return approximated_signal, selected_indices\n",
    "\n",
    "# Perform OMP with RL on the dataset\n",
    "sparsity = 50  # Desired sparsity level\n",
    "ssim_scores = []\n",
    "for image in tqdm(images):\n",
    "    approximated_image, selected_indices = orthogonal_matching_pursuit_rl(dict_learner.components_.T, image, sparsity, agent)\n",
    "    reconstructed_image = np.dot(images.T[:, selected_indices], approximated_image[selected_indices])\n",
    "    ssim_score = ssim(image.reshape(8, 8), reconstructed_image.reshape(8, 8), data_range=16)\n",
    "    ssim_scores.append(ssim_score)\n",
    "\n",
    "# Calculate average SSIM\n",
    "average_ssim = np.mean(ssim_scores)\n",
    "\n",
    "print(\"Average SSIM:\", average_ssim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def orthogonal_matching_pursuit(dictionary, target_signal, sparsity):\n",
    "    num_atoms = dictionary.shape[1]\n",
    "    selected_indices = []\n",
    "    approximated_signal = np.zeros(num_atoms)\n",
    "    residual = target_signal\n",
    "\n",
    "    for _ in range(sparsity):\n",
    "        correlations = np.abs(np.dot(dictionary.T, residual))\n",
    "        correlations[selected_indices] = 0  # Exclude already selected indices\n",
    "        selected_index = np.argmax(correlations)\n",
    "        selected_indices.append(selected_index)\n",
    "\n",
    "        selected_atom = dictionary[:, selected_index]\n",
    "        selected_atom_norm = np.linalg.norm(selected_atom)**2\n",
    "        approximated_signal[selected_index] += np.dot(selected_atom.T, target_signal) / selected_atom_norm\n",
    "\n",
    "        residual = target_signal - np.dot(dictionary[:, np.array(selected_indices)], approximated_signal[np.array(selected_indices)])\n",
    "\n",
    "    return approximated_signal, selected_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load MNIST dataset\n",
    "digits = load_digits()  # Replace with appropriate code to load MNIST dataset\n",
    "images = digits.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[6].reshape(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_learner = DictionaryLearning(\n",
    "    n_components=64,\n",
    "    fit_algorithm='cd',\n",
    "    transform_algorithm= 'omp', \n",
    "    random_state=0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_learner.fit(images[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim_scores = []\n",
    "\n",
    "for image in tqdm(images):\n",
    "    reconstructed_image = dict_learner.transform(image.reshape(1,-1)) \n",
    "    image = image.reshape(8,8) \n",
    "    reconstructed_image = reconstructed_image.reshape(8,8) \n",
    "    ssim_score = ssim(image, reconstructed_image, data_range=reconstructed_image.max() - reconstructed_image.min())\n",
    "    ssim_scores.append(ssim_score)\n",
    "\n",
    "# Calculate average SSIM\n",
    "average_ssim = np.mean(ssim_scores)\n",
    "\n",
    "print(\"Average SSIM:\", average_ssim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(reconstructed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# OMP parameters\n",
    "sparsity = 50  # Desired sparsity level\n",
    "\n",
    "# Perform OMP on each image\n",
    "ssim_scores = []\n",
    "for image in tqdm(images):\n",
    "    approximated_image, selected_indices = orthogonal_matching_pursuit(dict_learner.components_.T, image, sparsity)\n",
    "    reconstructed_image = np.dot(images.T[:, selected_indices], approximated_image[selected_indices])\n",
    "    reconstructed_image = reconstructed_image.reshape(8,8) \n",
    "    image = image.reshape(8,8) \n",
    "    ssim_score = ssim(image, reconstructed_image, data_range=reconstructed_image.max() - reconstructed_image.min())\n",
    "    ssim_scores.append(ssim_score)\n",
    "\n",
    "# Calculate average SSIM\n",
    "average_ssim = np.mean(ssim_scores)\n",
    "\n",
    "print(\"Average SSIM:\", average_ssim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
